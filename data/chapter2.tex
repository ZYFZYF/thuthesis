% !TeX root = ../main.tex

\chapter{相关研究综述}
\label{cha:intro}
\section{时间序列异常检测技术}
时间序列异常检测是一个很经典的问题。传统方法主要基于统计，最常用也最高效的是基于3$\sigma$原则，基于历史的数据分布来确定当前数据的合理波动范围，它假设数据分布满足正态分布，因此不在$(\mu - 3\sigma,\mu + 3\sigma)$范围内的数据极有可能是异常值。但实际数据可能不符合正态分布的假设，具体使用的标准差倍数也很难统一。另一种常用的是ARIMA\footnote{Autoregressive Integrated Moving Average model，差分整合移动平均自回归模型}方法，该方法适用于平稳、少突降/突增的数据，通过前一段的时间的数据来预测下一个时刻的数据，然后通过比较预测值和真实值的差异来判断异常的发生，但该方法有7个参数需要确定，不同的时间序列数据难以找到一套自动化的流程来确定参数。

近年来，随着人工智能技术的发展，越来越多的深度学习技术被用到了这一领域。而鉴于异常检测这一问题的特殊性：标签难以获得以及难以穷尽所有的异常，想要得到所有的有标数据然后做一个二分类的有监督学习是不现实的，因此目前在异常检测这里领域无监督方法还是主流，也就是用正常的数据训练，在这个过程中学习到正常数据的模式，而根据测试数据中夹杂的异常不符合正常数据的模式这一特性将异常区分出来。而无监督的方法也有各种思路：Malhotra\cite{malhotra2015long}首次将LSTM用于时间序列异常检测这一领域，在假设一段时间内的预测误差符合高斯分布的情况下，通过预测数据与真实数据的误差来判断异常；An\cite{an2015variational}则利用VAE\footnote{Varitional AutoEncoder，变分自编码器}的特点对正常数据用重构的方法来做单点的异常检测，并且不同于用AE来做重构的方法，用重构误差来作为评判异常的标准，而是提出了重构概率的概念，相比AE鲁棒性更强；Xu\cite{xu2018unsupervised}则进一步将VAE用到了单条时间序列数据的异常检测上；Malhotra\cite{malhotra2016lstm}则将预测与重构结合起来，提出了基于LSTM的Encoder-Decoder模型，有效吸取了两者的优点，换了LSTM在面临某些无法预测的时间序列的时候表现差的问题；除了预测和重构之外，Ruff\cite{ruff2018deep}用神经网络来实现OC-SVM\footnote{One-Class Support Vector Machine，一分类支持向量机}来解决传统OC-SVM在面临高维数据时表现差的问题；Zong\cite{zong2018deep}则将AE与传统的GMM\footnote{Gaussian Mixture Model，高斯混合模型}模型结合起来，前者用来将数据压缩到较低维度，而后者则在低纬度下已经被证明是一个有效的检测模型；最近，Park\cite{park2018multimodal}将LSTM和VAE结合起来，规避了VAE本身并不是一个时序模型的特点，将其用于机器人的传感器数据异常检测上并获得了不错的效果；Su\cite{su2019robust}则进一步考虑了VAE中随机变量的时序性的特点，在ServerMachineDataset数据集下实现了目前为止最好的结果。

但时间序列异常检测方面一直面临一个难题，就是如何评估的问题。首先不能像单点异常一样直接计算$Precision$和$Recall$，因为其前后有相关性。Hundman\cite{DBLP:conf/kdd/HundmanCLCS18}使用的方法是将一整段异常作为一个异常来算$Precision$和$Recall$，但如何定义一段异常被检测到与否对每个算法来说也是难以确定的；Xu\cite{xu2018unsupervised}提出了一种简单的调整的方式，认为一段异常只要有一个点被检测到那么整段都认为被检测到，然后在这个基础上重新计算$Precision$和$Recall$，但该方法也存在不够准确的方法，而且往往评价指标会虚高，因为对于一段很长的异常来说，模型只要能够检测到最为异常的那一个点，就能获得整段的分数。Tatbul\cite{tatbul2018precision}则系统地研究了这个问题，并且提出了一种新的针对时间序列的$Precision$和$Recall$的计算方式，通过将预测异常和真实异常都表示为区间的方式，综合考虑到预测异常与真实异常的相交的大小、位置、唯一性带来的不同影响，为时间序列异常检测提供了一个较为合理的评价指标。

\section{复杂系统中的根因定位}

在云网络中的多点根因定位上，Lin\cite{lin2016automated}提出了异常传播路径的概念，将多点之间的关联边分为水平边和竖直边，其中虚拟机和虚拟机之间因为服务调用产生联系的为水平边，而由于共享一台物理机则是竖直边，在有了异常传播图的基础上到所有异常点的距离之和就作为评判一个点是根因的依据。Weng\cite{weng2018root}则针对公共云中的多级服务调用场景在此基础上做出了改进，在图中对边的类型进行了区分并且引入了边权，通过计算时间序列数据之间的相关性来作为边权，在追溯根因时也选择了用随机游走来更真实地模拟异常传播，实现了更加细致和具体场景下的根因分析。Wu\cite{wu2020microrca}则在根因定位这一步上使用了Personal PageRank来更加高效和准确的计算根因。

以往的方法大都是将时间序列数据之间的相关性作为边权来构建异常传播图，在此基础上来考虑异常的传播，很少将时间序列数据异常检测与根因定位工作结合起来。因此本文旨在将二者结合起来，提出一个新的构图方式，在此基础上运用随机游走类算法得到根因。

\section{小结}
通过分析已有的相关工作，我们可以看出一方面，在时间异常检测领域，深度学习的方法缺乏一个统一的标准来评价，因此本文旨在建立这样一个通用的框架，内部使用合理的评价方式对已有的算法或者新算法进行性能、速度等各方面综合的评价；另一方面，在根因定位上以往的方法多与异常检测的联系不是那么密切，我们希望能够将异常检测的结果作为异常传播图的一部分，在此基础上进行随机游走来获得根因定位的结果。

